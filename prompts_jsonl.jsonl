{"text": "<s>[INST] Что такое перцептрон? [/INST] Перцептрон - простейшая форма искусственной нейронной сети, разработанная Фрэнком Розенблаттом в 1958 году. Состоит из входного слоя, весов связей, функции суммирования и активации (ступенчатая или сигмоидная). Используется для бинарной классификации, обучается по правилу дельта-правило.</s>"}
{"text": "<s>[INST] Что такое backpropagation? [/INST] Backpropagation (обратное распространение ошибки) - алгоритм обучения многослойных нейронных сетей. Вычисляет градиенты функции потерь по всем весам сети через цепное правило дифференцирования. Позволяет эффективно обучать глубокие сети методом градиентного спуска.</s>"}
{"text": "<s>[INST] Что такое функция активации ReLU? [/INST] ReLU (Rectified Linear Unit) - функция активации f(x) = max(0, x). Преодолевает проблему vanishing gradient у сигмоиды, ускоряет обучение в 6 раз, проста для вычисления. Недостаток: dying ReLU проблема, когда нейроны \"умирают\" и перестают обновляться.</s>"}
{"text": "<s>[INST] Что такое сверточная нейронная сеть CNN? [/INST] CNN (Convolutional Neural Network) - специализированная архитектура для обработки изображений. Использует сверточные слои для извлечения локальных признаков, пулинг для уменьшения размерности, полностью связанные слои для классификации. Применяется в компьютерном зрении.</s>"}
{"text": "<s>[INST] Что такое RNN и LSTM? [/INST] RNN (Recurrent Neural Network) - сеть с обратными связями для последовательных данных. LSTM (Long Short-Term Memory) - улучшенная RNN с ячейками памяти, гейтами (forget, input, output). Решает проблему vanishing gradient, эффективно моделирует долгосрочные зависимости в текстах и временных рядах.</s>"}
{"text": "<s>[INST] Что такое градиентный спуск? [/INST] Градиентный спуск - оптимизационный алгоритм минимизации функции потерь. Итеративно обновляет веса: w = w - η * ∇L(w), где η - learning rate, ∇L - градиент. Варианты: SGD (стохастический), Mini-batch GD, Adam, RMSprop.</s>"}
{"text": "<s>[INST] Что такое overfitting в нейронных сетях? [/INST] Overfitting - переобучение, когда модель идеально запоминает тренировочные данные, но плохо работает на новых. Признаки: высокая точность train, низкая validation. Решения: Dropout, L2/L1 регуляризация, early stopping, data augmentation, больше данных.</s>"}
{"text": "<s>[INST] Что такое трансформер? [/INST] Трансформер - архитектура без рекуррентных связей, основана на self-attention. Состоит из энкодера и декодера, параллельно обрабатывает все токены. Масштабируется до миллиардов параметров, основа GPT, BERT, T5. Введена в статье \"Attention is All You Need\" (2017).</s>"}
{"text": "<s>[INST] Что такое self-attention механизм? [/INST] Self-attention - механизм, позволяющий модели взвешивать важность разных частей входной последовательности. Вычисляет Query, Key, Value матрицы: Attention(Q,K,V) = softmax(QK^T / √d)V. Позволяет захватывать долгосрочные зависимости независимо от расстояния.</s>"}
{"text": "<s>[INST] Что такое GAN нейронные сети? [/INST] GAN (Generative Adversarial Network) - состязательные сети: генератор создает фейковые данные, дискриминатор отличает реальные от фейковых. Обучаются одновременно в игре с нулевой суммой. Применение: генерация изображений, deepfakes, StyleGAN.</s>"}
{"text": "<s>[INST] Что такое Batch Normalization? [/INST] Batch Normalization нормализует входы каждого слоя: μ,σ вычисляются по батчу, затем z = (x-μ)/σ, y = γz+β. Ускоряет обучение, стабилизирует градиенты, уменьшает внутренний сдвиг ковариации. Работает как регуляризация.</s>"}
{"text": "<s>[INST] Что такое Adam оптимизатор? [/INST] Adam (Adaptive Moment Estimation) сочетает Momentum и RMSprop. Поддерживает экспоненциально затухающие средние первого (моментум) и второго (RMS) моментов градиентов. Адаптивно изменяет learning rate для каждого параметра: m̂t, v̂t, θt+1 = θt - η m̂t/(√v̂t+ε).</s>"}
{"text": "<s>[INST] Что такое transfer learning? [/INST] Transfer learning - использование предобученной модели для новой задачи. Замораживают базовые слои (общие признаки), дообучают верхние (специфичные для задачи). Эффективно при малом объеме данных: ImageNet -> медицинские изображения.</s>"}
{"text": "<s>[INST] Что такое vanishing gradient problem? [/INST] Vanishing gradient - затухание градиентов при backpropagation в глубоких сетях. Происходит из-за умножения градиентов <1 (сигмоида, tanh). Решения: ReLU, LSTM/GRU, BatchNorm, остаточные связи (ResNet), градиентное накопление.</s>"}
{"text": "<s>[INST] Что такое автоэнкодер? [/INST] Автоэнкодер - сеть, обучаемая восстанавливать вход на выходе: encoder(x)→z→decoder(z)=x. Latent space z - компактное представление. Типы: denoising (шумоподавление), variational (VAE для генерации), sparse. Применение: сжатие, feature learning.</s>"}
{"text": "<s>[INST] Что такое positional encoding в трансформерах? [/INST] Positional encoding добавляет информацию о позиции токенов в self-attention (параллельная обработка). Синусоидальные функции: PE(pos,2i)=sin(pos/10000^2i/d), PE(pos,2i+1)=cos(pos/10000^2i/d). Позволяет модели различать порядок слов.</s>"}
{"text": "<s>[INST] Что такое multi-head attention? [/INST] Multi-head attention - параллельные attention головы: каждая обучает разные подпространства признаков. Concatenate outputs всех голов, линейная проекция. Улучшает выразительность: h = Concat(head1,…,headh)WO, где каждая headi=Attention(Qi,Ki,Vi).</s>"}
{"text": "<s>[INST] Что такое ResNet архитектура? [/INST] ResNet (Residual Network) вводит skip-связи (residual blocks): y = F(x) + x. Решает vanishing gradient в глубоких сетях (152 слоя). Identity mapping сохраняет информацию, градиенты текут напрямую. Победитель ImageNet 2015.</s>"}
{"text": "<s>[INST] Что такое dropout регуляризация? [/INST] Dropout - случайное отключение нейронов во время обучения (p=0.5 обычно). Предотвращает коадаптацию признаков, действует как ансамбль экспоненциально многих сетей. На инференсе масштабирует веса: w' = w(1-p). Применяется после активаций.</s>"}
{"text": "<s>[INST] Что такое BERT модель? [/INST] BERT (Bidirectional Encoder Representations from Transformers) - двунаправленный трансформер энкодер. Предобучается на masked LM + next sentence prediction. Fine-tuning на downstream задачах NLP. 12/24 слоя, 110M/340M параметров. Революция в NLP 2018.</s>"}
{"text": "<s>[INST] Что такое GPT архитектура? [/INST] GPT (Generative Pre-trained Transformer) - односторонний (left-to-right) трансформер декодер. Автореgressive генерация текста. Масштабирование: GPT-3 (175B параметров). Few-shot learning через in-context learning без fine-tuning.</s>"}
{"text": "<s>[INST] Что такое функция потерь cross-entropy? [/INST] Cross-entropy loss измеряет расхождение между истинными метками и предсказанными вероятностями: L = -∑yi log(ŷi). Стандарт для классификации. Градиент: ∂L/∂zi = ŷi - yi. Minimizируя CE, максимизируем likelihood.</s>"}
{"text": "<s>[INST] Что такое learning rate scheduler? [/INST] Learning rate scheduler динамически изменяет η во время обучения. Типы: StepLR (шаговое уменьшение), ReduceLROnPlateau (по плато validation), CosineAnnealing (косинусное затухание), Linear warmup. Улучшает сходимость и качество.</s>"}
{"text": "<s>[INST] Что такое attention mask? [/INST] Attention mask - матрица, блокирующая внимание к padding токенам или будущим позициям (в декодере). -inf для маскируемых позиций в softmax(QK^T). Обеспечивает causal attention и корректную обработку разной длины последовательностей.</s>"}
{"text": "<s>[INST] Что такое VAE (вариационный автоэнкодер)? [/INST] VAE - вероятностный автоэнкодер, моделирует p(x|z)p(z). Encoder выводит μ,σ → z~N(μ,σ), decoder p(x|z). Регуляризационный член KL(q(z|x)||p(z)). Генерирует новые данные из latent space, основа многих генеративных моделей.</s>"}
{"text": "<s>[INST] Что такое градиентное накопление? [/INST] Gradient accumulation - имитация больших батчей при ограниченной памяти GPU. Делаем несколько forward-backward проходов с малым батчем, суммируем градиенты, обновляем веса. Эквивалентно большому батчу: effective_batch = batch_size × accumulation_steps.</s>"}
{"text": "<s>[INST] Что такое Layer Normalization? [/INST] Layer Normalization нормализует по признакам (hidden dimension), не по батчу: μ,σ по всем нейронам слоя. Стабилизирует обучение трансформеров, независима от batch size. Применяется перед attention и FFN: LN(x) = γ(x-μ)/σ + β.</s>"}
{"text": "<s>[INST] Что такое diffusion models? [/INST] Diffusion models - генеративные модели, добавляющие шум к данным (forward diffusion), затем обучающие denoising сеть (reverse diffusion). DDPM, Stable Diffusion. Лучше GAN по качеству, параллельная генерация, controllable текстом (CLIP).</s>"}
{"text": "<s>[INST] Что такое mixed precision training? [/INST] Mixed precision training использует FP16/BF16 для вычислений, FP32 для градиентов и master weights. Снижает память в 2 раза, ускоряет на 3x (Tensor Cores). AMP (Automatic Mixed Precision) в PyTorch: torch.cuda.amp.GradScaler.</s>"}
{"text": "<s>[INST] Что такое Vision Transformer ViT? [/INST] ViT (Vision Transformer) применяет трансформер к изображениям: patch embedding (16x16→D), positional encoding, стандартный трансформер энкодер + classification head. Масштабируется лучше CNN при больших данных, ImageNet state-of-the-art.</s>"}
